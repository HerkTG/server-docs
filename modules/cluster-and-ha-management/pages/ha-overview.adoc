:toc:
= High Availability Overview
:description: Overview of High Availability functionality and supported features.

A TigerGraph system with High Availability (HA) is a cluster of server nodes which uses replication to provide continuous operation of most services in the event that one or more nodes is offline.

For example, an application's predefined queries will continue to run and file-based loading will continue with files resident at any of the working nodes.

Incorporating HA can yield a number of other system benefits, such as:

* The query workload is distributed across all replicas.
* Data loading operations are distributed across all nodes.
* Individual nodes can fail without impacting query workloads.
* Individual nodes can be re-paved without impacting query workloads.

NOTE: Re-pavement of a node, is the process that takes a node offline intentionally for maintenance or updates and then is brought back online.

== HA Considerations

TigerGraph HA provides continuous operation of some but not all services.
Please note the following exceptions and consider your options for taking additional steps to maintain continuous operation or to restore service as quickly as possible.

If an HA system is operating with a failed node, unless the system is reconfigured to xref:tigergraph-server:cluster-and-ha-management:crr-index.adoc#_exclusions[exclude that node], the following services are limited or unavailable:

* A file-based data load cannot be loaded.
* A data partition slated for a kafka-based data load cannot be loaded.
* New queries cannot be installed.
* Schema changes are not allowed.
* Backup and export operations are not available and will be rejected.

NOTE: As a workaround, full operation can be restored temporarily by the xref:_removal_of_failed_nodes[removal of the failing nodes].
For example, a 5 x 2 cluster with one node removed would become a 5 x 4 cluster, with only partial replication.

== High Availability Cluster Configuration

High Availability (HA) xref:tigergraph-server:cluster-and-ha-management:ha-cluster.adoc[Cluster Configuration] provides a resilient framework with flexibility for resizing and optimizing data distribution in TigerGraph systems.

Key factors include:

* The replication factor.
* The partitioning factor.
* The cluster size.

== High Availability Support for GSQL Server

High Availability (HA) support for xref:tigergraph-server:cluster-and-ha-management:ha-for-gsql-server.adoc[GSQL Server] ensures robustness by allowing components to take over in case of unavailability.
Incorporating a primary GSQL server and standby servers, collectively referred to as leaders and followers, offering HA for client connections.

NOTE: It is recommended to couple this with a client-side retry logic, so the failing user query will complete once the system adjusts to accommodate the failed node (typically up to 30 seconds).

== High Availability Support for Application Server

TigerGraph incorporates native High Availability (HA) support for its xref:tigergraph-server:cluster-and-ha-management:ha-for-application-server.adoc[application server], ensuring uninterrupted service for GraphStudio and Admin Portal APIs.
The active-active architecture runs on the first three nodes by default, in case of server failure, users can switch to an available server within the cluster.

NOTE: If the primary node is offline, access to GraphStudio is interrupted, but resumes once the primary node is back online.

In the event of a server failure, users can transition to the next available server within the cluster to resume operations,
with a reminder that any ongoing long-running operations will be lost.

.Additionally, load balancing supports the following services:
* Nginx
* AWS Elastic Load Balancer
* Azure Application Gateway
* GCP External HTTP(s) Load Balancer

== Cluster Commands

xref:tigergraph-server:cluster-and-ha-management:cluster-commands.adoc[Advanced Linux commands] that simplify platform operations can performed by the platform owner during debugging on HA clusters.

== Removal of Failed Nodes

HA cluster configuration allows for the xref:tigergraph-server:cluster-and-ha-management:remove-failed-node.adoc[removal of failed nodes] without data loss, facilitating cluster maintenance and recovery.
This feature is especially useful when addressing individual node failures or handling multiple concurrent node failures in a prepared environment, covering:

* Removing a Single Node
* Removing Multiple Nodes