= Data Streaming Connector
:description: A guide to TigerGraph's Streaming Data Connector.
:sectnums:

TigerGraph Data Streaming Connector is a link:https://docs.confluent.io/home/connect/overview.html[Kafka connector] that provides fast and scalable data streaming between TigerGraph and other data systems.

== Supported data systems
The data streaming connector supports the following data systems:

* Google Cloud Storage (GCS)

== Stream data from Google Cloud Storage
You can create a data connector between TigerGraph's internal Kafka server and your GCS service with a specified topic.
The connector will stream data from the data source in your GCS buckets to TigerGraph's internal Kafka cluster.
You can then create and run a loading job using the Kafka loader from Kafka into the graph store.

=== Prerequisites
* Your source data files are stored in Google Cloud Storage buckets.
* You have link:https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating[generated a key that has access to your source data from your Google Cloud Platform service account].

=== Procedure

==== Specify connector configurations
The connector configurations provide the following information:

* Connector class
* Your GCP service account credentials
* Information on how to parse the source data
* Mapping between connector and source file

[discrete]
==== Specify connector class
The connector class indicates what type of connector the configuration file is used to create.
Connector class is specified by the `connector.class` key.
For connecting to Google Cloud Filestore, its value is set to `com.tigergraph.kafka.connect.filesystem.FsSourceConnector`.

[discrete]
==== Provide GCP account credentials
You need to provide following information for the connector to access the files in your GCP Filestore bucket:

* `file.reader.settings.fs.gs.auth.service.account.email`: The email address associated with the service account
* `file.reader.settings.fs.gs.auth.service.account.private.key.id`: The private key ID associated with the service account used for GCS access.
* `file.reader.settings.fs.gs.auth.service.account.private.key`: The private key associated with the service account used for GCS access.
* `file.reader.settings.client_email`:
* `file.reader.settings.fs.gs.project.id`: Google Cloud Project ID with access to GCS buckets.
* `file.reader.settings.fs.gs.auth.service.account.enable`: Whether to create objects for the parent directories of objects with `/` in their path. For example, creating `gs://bucket/foo/` upon deleting or renaming `gs://bucket/foo/bar`.
* `file.reader.settings.fs.gs.impl`: The FileSystem for gs: (GCS) uris.
The value for this configuration should be `com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem`.
* `file.reader.settings.fs.AbstractFileSystem.gs.impl`: The AbstractFileSystem for `gs:` (GCS) URIs. Only necessary for use with Hadoop 2.
The value for this configuration should be `com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS`.

[discrete]
==== Specify parsing rules
The streaming connector supports the following file types:

* CSV files
* directories
* tar files
* zip files

For URIs that point to directories and compressed files, the connector will look for all CSV files inside the directory or the compressed file.
If you set `file.recursive`, the connector will look for CSV files recursively.

The following parsing options are available:


|===
|Name |Description |Default

|`file.regexp`
|The regular expression to filter which files to read.
The default value matches all files.
|`.*`

|`file.recursive`
|Whether to retrieve files recursively if the URI points to a directory.
|`true`

|`file.reader.type`
|The type of file reader to use.
The only supported value is `text`.
|`text`

|`file.reader.delimited.separator`
|The character that separates columns.
|`,`

|`file.reader.delimited.quote`
a|The explicit boundary markers for string tokens, either single or double quotation marks.

The parser will not treat separator characters found within a pair of quotation marks as a separator.

Accepted values:

* `single`: single quotes `'` are boundary markers.
* `double`: double quotes `"` are boundary markers.
* Empty string `""`: Quotation marks are treated as ordinary characters.
| Empty string `""`

|`file.reader.delimited.value.default`
|The default value for a column when its value is null.
| Empty string `""`

|`file.reader.batch.size`
|The maximum number of lines to include in a single batch.
|`1000`

|`file.reader.text.eol`
|End of line character.
|`\n`

|`file.reader.text.header`
|Whether the first line of the files is a header line.
|`false`

|`file.reader.text.archive.type`
a|File type for archive files.
Setting the value of this configuration to `auto` will allow the connector to decide file types automatically based on the file extensions.
Accepted values:

* `tar`
* `zip`
* `gzip`
* `none`
* `auto`
|`auto`

|`file.reader.text.archive.extensions.tar`
|If a file has this extension, treat it as a tar file
|`tar`

|`file.reader.text.archive.extensions.zip`
|If a file has this extension, treat it as a zip file
|`zip`

|`file.reader.text.archive.extensions.gzip`
|If a file has this extension, treat it as a gzip file
|`gz`, `tgz`
|===


[discrete]
==== Map source file to connector
The below configurations are required:

|===
|Name |Description |Default

| `name`
| Name of the connector.
| None. Must be provided by the user.

| `topic`
| Name of the topic to create in Kafka.
| None. Must be provided by the user.

|`tasks.max`
|The maximum number of tasks which can run in parallel.
|1

|`file.uris`
|The path(s) to the data files on Google Cloud Filestore.
The URI may point to a CSV file, a zip file, a gzip file, or a directory
|None.
Must be provided by the user.
|===

Below is an example configuration:

[,text]
----
connector.class=com.tigergraph.kafka.connect.filesystem.FsSourceConnector
file.reader.settings.fs.gs.auth.service.account.email=gcsconnect@tigergraph-dev.iam.gserviceaccount.com
file.reader.settings.fs.gs.auth.service.account.private.key.id=55c1d79a46c1f3f59ef72e0df53285a3eef8ec38
file.reader.settings.fs.gs.auth.service.account.private.key="-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDSqbYRwD68FvA7\nLkC1HpjrJ9QIJ+iOyQPFeSoI+3pjmVTrX2B2aYIMByNubV6Js+n1x5ro/XW0nt3y\nk/BhdOTXj7JVRj6JMIb0yjsRQMi3J+3yOb2EFVHUDQ+4nmTuSJsdiOI1mh1pFN+Q\nXdvHP5hOwCaB4Pb/X7ya9YOokW3dVqbHtj/DO3l+rDhqEP0SH4+RInFbZon1AT3J\ncWDdMTsx4yW1PQNERzP/9M34du3ihWeT1xLLXquhMnFO+zECuPsoz1jFQrLCAFeX\nQSBx0/NgBCRqEsX4XESQ43bB4mD3D9AvfOc6IuYqKBcjG2HmmjOvidmnlRUgZJy/\n2rymIUXnAgMBAAECggEAR6itI0qmzGptG2R3ZGTdFZi9umyA4hkkrEaz8sxAbKLa\nzRnrgTwQnbDL76NKdkL6Ab39RuX45RDpZLvIGA6gTWc2/WTgnuAf+CLWht7np83w\nVeYoPkbWR/CNeXp/0MJn6VsHv74F5RnRlpUmzpcmYxtfvexdeK8DRB7hwzR9D73t\nCyad7O6NZabuOQBrTMgKL+So6gurVjW+KB8S9vgMvULLOmTZ1iUpRlh89cJ4/jRh\n4ltV+4EvBJvIlXD2GKMGRw8d/YPWmETO/dpz0aAs3sMgXdiFV3SAjAn8BgaYita8\nIAhLLOf/kFmFmmlM2k02iAZPIBjFvAs7ChGEHsXecQKBgQDq9AKPXaMOiy43EzHP\nU1cDKf8mwk9ELtfGPhySG9Z+zUclvoJnbq0XpP3cKJWgKILtcG+cUoYLSWrosE2i\n7W1976EwObNgg22CEICWclE0Mc2vMXkJT+ZUoqWWK5n83ZK+R05AqsBhWC9FLTQB\nBD5XlOnCUKQT3+3TRkNAB5I/6QKBgQDliKxQ2TR8OEkgII8ugyhVAfKamE2nVrED\nW0U/IbBQUaTk1niZbzROLRRfgqYqtLM0vEfMkKwiMDinmXsuurw2IOZaGbEyNFZT\nLpDXNjTTePJn192OT6wyRpDsuZNh+0uiXBFyJb8vRrMyWmtau9HXqD2kXOcKCesL\nVDfilsAFTwKBgCkKpsfUW39W4KPOPo0wyapL0745gw8t/5MplmQPaNCNmzgEp1La\nCnJu58llbX2klfpUAasU30Vpdbtf0K/9OXseONHrwmHBk4d8ynl9TqIHcR6BTdtK\nkbmHD9XDmAqLye5jFlBFg4V9mgRDeSoUS6+Q26SN4Zt3KlwVkfnFWM7BAoGBAJnr\nA3oXnQ1rhQXJL5qGEwamDrRCS1haVsskahQCmEPT69oUQ7zICHAf5JiDeMAMeltz\nokX4AaXPZj5lOmhEii9V8oIa1msPE5AmGrRmQhhI82xVIdnrbVItZcOIUd+Tbs2K\nJZzA2Spvo3yxi2nFptqRk/xi2/8sVXQ8XllQs6UbAoGAdqnrlEAIlCb5hdVNrLXT\nToqdq54G9g82L8/Y+WraqJSNOFKXCGQvC2N16ava4sZ65DCjT6FnCR/UhYS7Z6Vf\nR5EtMRYAyAcyn3g9tcfzINmEbpvwpHBqsr1xPcrfx/WRurIC6EBgLPgX+lALBI0G\n+Uu87tgHhcGFJfmQMQNeQWM=\n-----END PRIVATE KEY-----\n"
file.reader.settings.client_email="gcsconnect@tigergraph-dev.iam.gserviceaccount.com"
file.reader.settings.fs.gs.project.id="tigergraph-dev"
file.reader.settings.fs.gs.auth.service.account.enable=true
file.reader.settings.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
file.reader.settings.fs.AbstractFileSystem.gs.impl="com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"

mode=eof
file.regexp=".*"
file.recursive=true
file.reader.type=text
file.reader.batch.size=10000
file.reader.text.eol="\\n"
file.reader.text.header=true
file.reader.text.archive.type=auto
file.reader.text.archive.extensions.tar=tar
file.reader.text.archive.extensions.zip=zip
file.reader.text.archive.extensions.gzip=tar.gz,tgz

[connector_person]
name = fs-person-demo-104
tasks.max=10
topic=person-demo-104
file.uris=gs://tg_csv/p.csv

[connector_friend]
name = fs-friend-demo-104
tasks.max=10
topic=friend-demo-104
file.uris=gs://tg_csv/f.csv
----

==== Create connector
Run command `gadmin connector create` and specify the configuration file to create the connector:

  gadmin connector create --c <config_file>

The connectors start streaming from the data source immediately after creation if the configurations are valid.
You can run `gadmin connector status` to verify the status of the connectors.
If the configurations are valid, the connectors should be in `RUNNING` status.

Data streamed from the source will stay in TigerGraph's internal Kafka topics until they are ingested by a loading job.

==== Create data source
Now that the connector has started loading data into TigerGraph's internal Kafka cluster, you can create a data source and point it to the Kafka cluster:

. Create a data source configuration file.
The broker's IP and hostname should be `localhost:30002`, which points to the port for TigerGraph's internal Kafka cluster.
In the `kafka.config` field, set `group.id` to `tigergraph`:
+
[,json]
----
{
	"broker":"localhost:30002",
    "kafka_config":
        {
            "group.id": "tigergraph"
        }
}
----
. Run `CREATE DATA SOURCE` to create the data source:
+
[,gsql]
----
CREATE DATA_SOURCE KAFKA k1 FOR GRAPH social
----

==== Create loading job
Create a loading job to load data from the data source:

. Create a topic-partition configuration for each topic.
+
[,javascript]
----
{
  "topic": <topic_name>, <1>
  "partition_list": [ <2>
    {
      "start_offset": <offset_value>, <3>
      "partition": <partition_number> <4>
    },
    {
      "start_offset": <offset_value>, <3>
      "partition": <partition_number> <4>
    }
    ...
  ]
}
----
<1> Replace `<topic_name>` with the name of the topic this configuration applies to.
<2> List of partitions you want to stream from.
For each partition, you can set a start offset.
If this list is empty, or `partition_list` isn't included, all partitions are used with the default offset.
<3> Replace `<offset_value>` with the offset value you want.
The default offset for loading is `-1`, which means you will load data from the most recent message in the topic.
If you want to load from the beginning of a topic, the `start_offset` value should be `-2`.
<4> Replace `<partition_number>` with the partition number if you want to configure.
. Create a loading job and map data to graph.
See xref:kafka-loader-user-guide.adoc#_2_create_a_loading_job[Kafka loader guide] for how to map data from a Kafka data source to the graph.

For example, suppose we have the following two CSV files and schema:

[tabs]
====
Schema::
+
--
[,gsql]
----
CREATE VERTEX person (PRIMARY_ID name STRING, name STRING, age INT, gender STRING, state STRING)
CREATE UNDIRECTED EDGE friendship (FROM person, TO person, connect_day DATETIME)
CREATE GRAPH social (person, friendship)
----
--
p.csv::
+
--
[,csv]
----
name,gender,age,state
Tom,male,40,ca
Dan,male,34,ny
Jenny,female,25,tx
Kevin,male,28,az
Amily,female,22,ca
Nancy,female,20,ky
Jack,male,26,fl
A,male,29,ca
----
--
f.csv::
+
--
[,csv]
----
person1,person2,date
Tom,Dan,2017-06-03
Tom,Jenny,2015-01-01
Dan,Jenny,2016-08-03
Jenny,Amily,2015-06-08
Dan,Nancy,2016-01-03
Nancy,Jack,2017-03-02
Dan,Kevin,2015-12-30
Amily,Dan,1990-1-1
----
--
====


The following topic-partition configurations and loading job will load the two CSV files into the graph:

[tabs]
====
topic_person.json::
+
--
[,json]
----
{
  "topic": "person-demo-104",
  "partition_list": [
    {
      "start_offset": -2,
      "partition": 0
    }
  ]
}
----
--
topic_friend.json::
+
--
[,json]
----
{
  "topic": "friend-demo-104",
  "partition_list": [
    {
      "start_offset": -2,
      "partition": 0
    }
  ]
}
----
--
Loading job::
+
--
[.wrap,gsql]
----
CREATE LOADING JOB load_person FOR GRAPH social {
    DEFINE FILENAME f1 = "$k1:/home/mydata/topic_person.json";
    DEFINE FILENAME f2 = "$k1:/home/mydata/topic_friend.json";
    LOAD f1 TO VERTEX person VALUES ($0, $0, $2, $1, $3) USING separator=",";
    LOAD f2 TO EDGE friendship VALUES ($0, $1, $2)  USING separator=",";
}
----
--
====

==== Run loading job
Run the loading job created in the last step will load the streamed data into the graph.
If you make changes to the topic-partition configuration file, you can overwrite the values for the filename variables with xref:gsql-ref:ddl-and-loading:running-a-loading-job.adoc#_run_loading_job[the `USING` clause].

[,gsql]
----
GSQL > RUN LOADING JOB load_person
----

By default, loading jobs that use Kafka data sources run in streaming mode and do not stop until manually aborted.
As data is streamed from the data source, the running loading job will continuously ingest the streamed data into the graph store.

==== Stream data source updates

To load updates to the data source after you've created the connectors, you need to delete the connector and recreate another connector.

In the future, the streaming data connector will support automatically scanning for updates and stream data to TigerGraph.

== Manege connectors

After creating a connector, you can choose to delete it or pause it.
You can also use `gadmin` commands to view status of all existing connectors.

=== List connectors
You can list all running connectors or view detailed configuration of a specific connector.

To view a list of all connectors, run `gadmin connector list` from your bash terminal as the TigerGraph Linux user.

To view configuration details of a specific connector, run `gadmin connector list <name_of_connector>` and replace `<name_of_connector>` with the name of the connector you want to inspect.

=== Pause a connector
Pausing a connector stops the connector from streaming data from the data source.
The data that has been streamed to TigerGraph's internal Kafka can still be loaded into the graph store via a loading job.

To pause a connector, run the below command and replace `<connector_name>` with the name of the connector:

[,console]
----
$ gadmin connector pause <connector_name>
----

=== Resume a connector
Resuming a connector resumes streaming for a paused connector.

To resume a connector,  run the below command and replace `<connector_name>` with the name of the connector:

[,console]
----
$ gadmin connector resume <connector_name>
----

=== Delete a connector
Deleting a connector removes a connector.
It stops the connector from streaming, but the data that has been streamed to Kafka can still be ingested by a loading job.
This action cannot be undone and a removed connector cannot be resumed.

To delete a connector,  run the below command and replace `<connector_name>` with the name of the connector:

[,console]
----
$ gadmin connector delete <connector_name>
----