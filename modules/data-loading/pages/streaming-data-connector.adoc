= Streaming Data Connector
:description: A guide to TigerGraph's Streaming Data Connector.
:sectnums:

TigerGraph Streaming Data Connector provides fast and scalable data streaming between TigerGraph and other data systems.

== Supported data systems
The streaming data connectors supports the following data systems:

* Google Cloud Filestore

== Stream data from Google Cloud File Storage
You can create a data connector between TigerGraph's internal Kafka server and your Google Cloud Filestore with a specified topic.
Having created the connector, you can create a loading job using the Kafka loader.
Running the loading job will pull data from the source into TigerGraph.

=== Prerequisites
* Your source data files are stored in Google Cloud Filestore bucket.
* You have link:https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating[generated a key that has access to your source data from your Google Cloud Platform service account].

=== Procedure

==== Specify connector configurations
The connector configurations provide the following information:

* Connector class
* Your GCP service account credentials
* Information on how to parse the source data
* Mapping between connector and source file

[discrete]
==== Specify connector class
The connector class indicates what type of connector the configuration file is used to create.
Connector class is specified by the `connector.class` key.
For connecting to Google Cloud Filestore, its value is set to `com.tigergraph.kafka.connect.filesystem.FsSourceConnector`.

[discrete]
==== Provide GCP account credentials
You need to provide following information for the connector to access the files in your GCP Filestore bucket:

* `file.reader.settings.fs.gs.auth.service.account.email`: The email address associated with the service account
* `file.reader.settings.fs.gs.auth.service.account.private.key.id`: The private key ID associated with the service account used for GCS access.
* `file.reader.settings.fs.gs.auth.service.account.private.key`: The private key associated with the service account used for GCS access.
* `file.reader.settings.client_email`:
* `file.reader.settings.fs.gs.project.id`: Google Cloud Project ID with access to GCS buckets.
* `file.reader.settings.fs.gs.auth.service.account.enable`: Whether to create objects for the parent directories of objects with `/` in their path. For example, creating `gs://bucket/foo/` upon deleting or renaming `gs://bucket/foo/bar`.
* `file.reader.settings.fs.gs.impl`: The FileSystem for gs: (GCS) uris.
The value for this configuration should be `com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem`.
* `file.reader.settings.fs.AbstractFileSystem.gs.impl`: The AbstractFileSystem for `gs:` (GCS) URIs. Only necessary for use with Hadoop 2.
The value for this configuration should be `com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS`.

[discrete]
==== Specify parsing rules
The streaming connector supports the following file types:

* CSV files
* directories
* tar files
* zip files

For URIs that point to directories and compressed files, the connector will look for all CSV files inside the directory or the compressed file.
If you set `file.recursive`, the connector will look for CSV files recursively.

The following parsing options are available:

|===
|Name |Description |Default

|`file.regexp`
|The regular expression to filter which files to read.
The default value matches all files.
|`.*`

|`file.recursive`
|Whether to retrieve files recursively if the URI points to a directory.
|`true`

|`file.reader.type`
|Column 2, row 5
|Column 3, row 5

|`file.reader.batch.size`
|Number of records to process at a time. Non-positive values disable batching.
|Column 3, row 6

|`file.reader.text.eol`
|End of line character.
|`\n`

|`file.reader.text.header`
|Whether the first line of the files is a header line.
|Column 3, row 8

|`file.reader.text.archive.type`
|File type for archive files.
Setting the value of this configuration to `auto` will allow the connector to decide file types automatically based on the file extensions.
|Column 3, row 9

|`file.reader.text.archive.extensions.tar`
|If a file has this extension, treat it as a tar file
|Column 3, row 10

|`file.reader.text.archive.extensions.zip`
|If a file has this extension, treat it as a zip file
|

|`file.reader.text.archive.extensions.gzip`
|If a file has this extension, treat it as a gzip file
|
|===


[discrete]
==== Map source file to connector
The below configurations are required:

|===
|Name |Description |Default

| `name`
| Name of the connector.
| None. Must be provided by the user.

| `topic`
| Name of the topic to create in Kafka.
| None. Must be provided by the user.

|`tasks.max`
|The maximum number of tasks which can run in parallel.
|1

|`file.uris`
|The path(s) to the data files on Google Cloud Filestore.
The URI may point to a CSV file, a zip file, a gzip file, or a directory
|None.
Must be provided by the user.
|===

Below is an example configuration:

[,text]
----
connector.class=com.tigergraph.kafka.connect.filesystem.FsSourceConnector
file.reader.settings.fs.gs.auth.service.account.email=gcsconnect@tigergraph-dev.iam.gserviceaccount.com
file.reader.settings.fs.gs.auth.service.account.private.key.id=55c1d79a46c1f3f59ef72e0df53285a3eef8ec38
file.reader.settings.fs.gs.auth.service.account.private.key="-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDSqbYRwD68FvA7\nLkC1HpjrJ9QIJ+iOyQPFeSoI+3pjmVTrX2B2aYIMByNubV6Js+n1x5ro/XW0nt3y\nk/BhdOTXj7JVRj6JMIb0yjsRQMi3J+3yOb2EFVHUDQ+4nmTuSJsdiOI1mh1pFN+Q\nXdvHP5hOwCaB4Pb/X7ya9YOokW3dVqbHtj/DO3l+rDhqEP0SH4+RInFbZon1AT3J\ncWDdMTsx4yW1PQNERzP/9M34du3ihWeT1xLLXquhMnFO+zECuPsoz1jFQrLCAFeX\nQSBx0/NgBCRqEsX4XESQ43bB4mD3D9AvfOc6IuYqKBcjG2HmmjOvidmnlRUgZJy/\n2rymIUXnAgMBAAECggEAR6itI0qmzGptG2R3ZGTdFZi9umyA4hkkrEaz8sxAbKLa\nzRnrgTwQnbDL76NKdkL6Ab39RuX45RDpZLvIGA6gTWc2/WTgnuAf+CLWht7np83w\nVeYoPkbWR/CNeXp/0MJn6VsHv74F5RnRlpUmzpcmYxtfvexdeK8DRB7hwzR9D73t\nCyad7O6NZabuOQBrTMgKL+So6gurVjW+KB8S9vgMvULLOmTZ1iUpRlh89cJ4/jRh\n4ltV+4EvBJvIlXD2GKMGRw8d/YPWmETO/dpz0aAs3sMgXdiFV3SAjAn8BgaYita8\nIAhLLOf/kFmFmmlM2k02iAZPIBjFvAs7ChGEHsXecQKBgQDq9AKPXaMOiy43EzHP\nU1cDKf8mwk9ELtfGPhySG9Z+zUclvoJnbq0XpP3cKJWgKILtcG+cUoYLSWrosE2i\n7W1976EwObNgg22CEICWclE0Mc2vMXkJT+ZUoqWWK5n83ZK+R05AqsBhWC9FLTQB\nBD5XlOnCUKQT3+3TRkNAB5I/6QKBgQDliKxQ2TR8OEkgII8ugyhVAfKamE2nVrED\nW0U/IbBQUaTk1niZbzROLRRfgqYqtLM0vEfMkKwiMDinmXsuurw2IOZaGbEyNFZT\nLpDXNjTTePJn192OT6wyRpDsuZNh+0uiXBFyJb8vRrMyWmtau9HXqD2kXOcKCesL\nVDfilsAFTwKBgCkKpsfUW39W4KPOPo0wyapL0745gw8t/5MplmQPaNCNmzgEp1La\nCnJu58llbX2klfpUAasU30Vpdbtf0K/9OXseONHrwmHBk4d8ynl9TqIHcR6BTdtK\nkbmHD9XDmAqLye5jFlBFg4V9mgRDeSoUS6+Q26SN4Zt3KlwVkfnFWM7BAoGBAJnr\nA3oXnQ1rhQXJL5qGEwamDrRCS1haVsskahQCmEPT69oUQ7zICHAf5JiDeMAMeltz\nokX4AaXPZj5lOmhEii9V8oIa1msPE5AmGrRmQhhI82xVIdnrbVItZcOIUd+Tbs2K\nJZzA2Spvo3yxi2nFptqRk/xi2/8sVXQ8XllQs6UbAoGAdqnrlEAIlCb5hdVNrLXT\nToqdq54G9g82L8/Y+WraqJSNOFKXCGQvC2N16ava4sZ65DCjT6FnCR/UhYS7Z6Vf\nR5EtMRYAyAcyn3g9tcfzINmEbpvwpHBqsr1xPcrfx/WRurIC6EBgLPgX+lALBI0G\n+Uu87tgHhcGFJfmQMQNeQWM=\n-----END PRIVATE KEY-----\n"
file.reader.settings.client_email="gcsconnect@tigergraph-dev.iam.gserviceaccount.com"
file.reader.settings.fs.gs.project.id="tigergraph-dev"
file.reader.settings.fs.gs.auth.service.account.enable=true
file.reader.settings.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
file.reader.settings.fs.AbstractFileSystem.gs.impl="com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"

mode=eof
file.regexp=".*"
file.recursive=true
file.reader.type=text
file.reader.batch.size=10000
file.reader.text.eol="\\n"
file.reader.text.header=true
file.reader.text.archive.type=auto
file.reader.text.archive.extensions.tar=tar
file.reader.text.archive.extensions.zip=zip
file.reader.text.archive.extensions.gzip=tar.gz,tgz

[connector_person]
name = fs-person-demo-104
tasks.max=10
topic=person-demo-104
file.uris=gs://tg_csv/p.csv

[connector_friend]
name = fs-friend-demo-104
tasks.max=10
topic=friend-demo-104
file.uris=gs://tg_csv/f.csv
----

==== Create connector
Run command `gadmin connector create` and specify the configuration file to create the connector:

  gadmin connector create --c test.cfg

The connectors start loading from the data source immediately after creation if the configurations are valid.
You can run `gadmin connector status` to verify the status of the connectors.
If the configurations are valid, the connectors should be in `RUNNING` status.

==== Create data source
Now that the connector has started loading data into TigerGraph's internal Kafka cluster, you can create a data source and point it to the Kafka cluster:

. Create a data source configuration file.
The broker's IP and hostname should be `localhost:30002`, which points to the port for TigerGraph's internal Kafka cluster.
In the `kafka.config` field, set `group.id` to `tigergraph`:
+
[,json]
----
{
	"broker":"localhost:30002",
    "kafka_config":
        {
            "group.id": "tigergraph"
        }
}
----
. Run `CREATE DATA SOURCE` to create the data source:
+
[,gsql]
----
CREATE DATA_SOURCE KAFKA k1 FOR GRAPH social
----

==== Create loading job
Create a loading job to load data from the data source:

. Create a topic-partition configuration for each topic.
+
[,javascript]
----
{
  "topic": <topic_name>, <1>
  "partition_list": [ <2>
    {
      "start_offset": <offset_value>, <3>
      "partition": <partition_number> <4>
    },
    {
      "start_offset": <offset_value>, <3>
      "partition": <partition_number> <4>
    }
    ...
  ]
}
----
<1> Replace `<topic_name>` with the name of the topic this configuration applies to.
<2> List of partitions you want to stream from.
For each partition, you can set a start offset.
If this list is empty, or `partition_list` isn't included, all partitions are used with the default offset.
<3> Replace `<offset_value>` with the offset value you want.
The default offset for loading is `-1`, which means you will load data from the most recent message in the topic.
If you want to load from the beginning of a topic, the `start_offset` value should be `-2`.
<4> Replace `<partition_number>` with the partition number if you want to configure.
. Create a loading job and map data to graph.
See xref:kafka-loader-user-guide.adoc#_2_create_a_loading_job[Kafka loader guide] for how to map data from a Kafka data source to the graph.

For example, suppose we have the following two CSV files and schema:

[tabs]
====
Schema::
+
--
[,gsql]
----
CREATE VERTEX person (PRIMARY_ID name STRING, name STRING, age INT, gender STRING, state STRING)
CREATE UNDIRECTED EDGE friendship (FROM person, TO person, connect_day DATETIME)
CREATE GRAPH social (person, friendship)
----
--
p.csv::
+
--
[,csv]
----
name,gender,age,state
Tom,male,40,ca
Dan,male,34,ny
Jenny,female,25,tx
Kevin,male,28,az
Amily,female,22,ca
Nancy,female,20,ky
Jack,male,26,fl
A,male,29,ca
----
--
f.csv::
+
--
[,csv]
----
person1,person2,date
Tom,Dan,2017-06-03
Tom,Jenny,2015-01-01
Dan,Jenny,2016-08-03
Jenny,Amily,2015-06-08
Dan,Nancy,2016-01-03
Nancy,Jack,2017-03-02
Dan,Kevin,2015-12-30
Amily,Dan,1990-1-1
----
--
====


The following topic-partition configurations and loading job will load the two CSV files into the graph:

[tabs]
====
topic_person.json::
+
--
[,json]
----
{
  "topic": "person-demo-104",
  "partition_list": [
    {
      "start_offset": -2,
      "partition": 0
    }
  ]
}
----
--
topic_friend.json::
+
--
[,json]
----
{
  "topic": "friend-demo-104",
  "partition_list": [
    {
      "start_offset": -2,
      "partition": 0
    }
  ]
}
----
--
Loading job::
+
--
[.wrap,gsql]
----
CREATE LOADING JOB load_person FOR GRAPH social {
    DEFINE FILENAME f1 = "$k1:/home/mydata/topic_person.json";
    DEFINE FILENAME f2 = "$k1:/home/mydata/topic_friend.json";
    LOAD f1 TO VERTEX person VALUES ($0, $0, $2, $1, $3) USING separator=",";
    LOAD f2 TO EDGE friendship VALUES ($0, $1, $2)  USING separator=",";
}
----
--
====

==== Run loading job
Run the loading job created in the last step will load data into the graph.
If you make changes to the topic-partition configuration file, you can overwrite the values for the filename variables with the `USING` clause.

[,gsql]
----
GSQL > RUN LOADING JOB load_person
----


