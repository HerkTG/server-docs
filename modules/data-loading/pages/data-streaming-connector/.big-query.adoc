= Stream from Google BigQuery
:description: Steps to query and streaming data from Google BigQuery using Data Streaming Connector.
:sectnums:

The Data Streaming Connector allows you to invoke SQL queries to your Google BigQuery dataset and stream the query results to TigerGraph’s internal Kafka server with a specified topic.
You can then create and run a xref:kafka-loader/index.adoc[Kafka loading job] to load data from Kafka into your graphs.

== Prerequisites
* You should have one of the following authentication credentials:
** Google Service Account credentials
** Access and refresh tokens

== Procedure

=== Specify connector configurations

The connector configurations provide the following information:

* Connector class
* Connection URL
* Value converter
* SQL query statement
* Connector properties

==== Specify connector class
The connector class indicates what type of connector the configuration file is used to create.
Connector class is specified by the `connector.class` key.

For streaming from Google BigQuery, the class is `io.confluent.connect.jdbc.JdbcSourceConnector`.

[source,text]
----
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
----

==== Specify connection URL

To stream data from BigQuery, Data Streaming Connector uses the https://cloud.google.com/bigquery/docs/reference/odbc-jdbc-drivers#current_jdbc_driver[BigQuery JDBC Driver].
The connection URL requires the domain name or IP address of the BigQuery server (`\https://www.googleapis.com/bigquery/v2`), the port number, the ID of your BigQuery project, and your credentials for authentication.
You can also supply other optional properties to the connection that are supported by https://storage.googleapis.com/simba-bq-release/jdbc/Simba%20Google%20BigQuery%20JDBC%20Connector%20Install%20and%20Configuration%20Guide_1.3.0.1001.pdf[the BigQuery JDBC connector]:

[source.wrap,text]
----
connection.url=jdbc:bigquery://https://www.googleapis.com/bigquery/v2:<port_number>;<property1>=<value1>;<property2>=<value2>;... <1>
----
<1> Each property-value pair is separated by a semicolon`;`.

The following is a list of required or frequently used properties:

|===
|Property |Description

|`ProjectId`
|ID of your Google Cloud Project for you BigQuery server.
Required.

|`OAuthType`
a|Required. A number that specifies the type of authentication used by the connector:

* `0`: Authentication with Google Service Account.
This requires that you supply two additional parameters:
** `OAuthServiceAcctEmail`
** `OAuthPvtKeyPath`
* `2`: Authentication with access token or refresh token.
This requires that you supply one of the two following parameters:
** `OAuthAccessToken`
** `OAuthRefreshToken`

|`OAuthServiceAcctEmail`
|Google Service Account Email used to authenticate the connection.
Required if you set `AuthValue` to `0`.

|`OAuthPvtKeyPath`
|Path to the key file that is used to authenticate the service account email address.
This parameter supports keys in .p12 or .json format.
Required if you set `AuthValue` to `0`.
|`OAuthAccessToken`
|Access token of your Google Service Account.
See Google Cloud documentation to https://cloud.google.com/iam/docs/create-short-lived-credentials-direct#sa-credentials-oauth[generate an access token].
Required if you set `AuthValue` to `2`.
|`OAuthRefreshToken`
|Refresh token of your Google Service Account.
See Google Cloud documentation to https://developers.google.com/identity/protocols/oauth2/web-server#exchange-authorization-code[generate a refresh token].
Required if you set `AuthValue` to `2`.
|===

==== Value converter
BigQuery stores data in a proprietary columnar format.
In order for TigerGraph to ingest the data, Data Streaming Connector uses a value converter to convert the data into the CSV format.
To do this, add a line in the configuration file that specifies the value converter:

[source.wrap,text]
value.converter=com.tigergraph.kafka.connect.converters.TigerGraphCsvConverter

By default, CSV values are separated by commas.
If you want to specify the separator, use the `value.converter.csv.separator` configuration to specify the separator.
For example, the following converter configurations use `|` as the separator.
If you change the separator, make sure to use the same separator when you create the loading job.

[source.wrap,text]
value.converter=com.tigergraph.kafka.connect.converters.TigerGraphCsvConverter
value.converter.csv.separator=|


==== Subsection configurations
Subsection configurations are specified after the overall connector configurations.
Each set of subsection configurations creates an instance of a connector by specifying a connector name and a Kafka topic that the connector streams messages to.

When streaming from BigQuery, each subsection also must have a SQL query statement used to query your BigQuery warehouse.
The results of the query are streamed to the corresponding topic.

NOTE: If you are querying `STRUCT` data or arrays, see <<_querying_struct_data_and_arrays>>.
`BYTES` type values are automatically converted to UTF-8 strings.

For example, the following subsection configurations create two instances of the connector, each streaming the results of their corresponding queries to `topic_0` and `topic_1`:

[source.wrap,text]
----
[bq_query_0]
name="bq_query_0"
topic="topic_0"
query="SELECT * FROM `bigquery-public-data.breathe.arxiv` LIMIT 1000"

[bq_query_1]
name="bq_query_1"
topic="topic_1"
query="SELECT doi,abstract,authors FROM `bigquery-public-data.breathe.nature`"
----

include::partial$create-connector.adoc[

include::partial$create-data-source.adoc[]

include::partial$create-loading-job-kafka.adoc[]

include::partial$run-loading-job.adoc[]

== Querying `STRUCT` data and arrays
If the record being queried in your BigQuery warehouse contains `STRUCT` data or arrays,  conversion functions need to be applied to the SQL statement:

=== `STRUCT` data

When retrieving struct data, it is recommended to retrieve the fields of the data directly.
For example:

[.wrap,sql]
----
SELECT basic.age, basic.gender FROM `project.dataset.table`
----

If you want to retrieve the entire `STRUCT`, you need to first use `TO_JSON_STRING()` to convert the `STRUCT` data into JSON strings in the converted CSV streams.

For example, `SELECT TO_JSON_STRING(col) FROM table`.
After converting the data into string format,  xref:gsql-ref:ddl-and-loading:creating-a-loading-job.adoc#_flatten_a_json_object_in_a_csv_file[flatten the JSON strings] when loading the data into the graph.

=== Arrays

To load array values, apply the function `ARRAY_TO_STRING` to the columns of `ARRAY` type.
For example, `SELECT ARRAY_TO_STRING(col_arr,separator) FROM table`.
It is important to ensure that the separator used here is distinct from the separator in your CSV streams.

After converting the array to strings, the string representation of the arrays will be in the CSV streams.
You can then xref:gsql-ref:ddl-and-loading:creating-a-loading-job.adoc#_loading_a_list_or_set_attribute[load the data in CSV as a list].

== Example

For example, suppose we have the following source table in BigQuery and the following graph schema in TigerGraph:

[tabs]
====
Source table::
+
--

|===
|name: String |basic: Struct |tags: Array<String> |state: String

|Tom
|{“age“:40, ”gender”:”male”}
|[“tall“,”strong”]
|ca

|Dan
|{“age“:35, ”gender”:”female”}
|[“smart“,”blonde”]
|ny
|===
--
Graph schema::
+
--
[.wrap,gsql]
----
CREATE VERTEX person (PRIMARY_ID name STRING, name STRING, age INT, gender STRING, state STRING, tags LIST<STRING>)
CREATE GRAPH social (person)
----
--
====

The following configuration invokes the provided query on the source table and convert the result to CSV format:

[source.wrap,text]
----
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
connection.url="jdbc:bigquery://https://www.googleapis.com/bigquery/v2:443;OAuthType=2;ProjectId=tigergraph;OAuthAccessToken=xxx;OAuthRefreshToken=yyy;OAuthClientId=zzz;OAuthClientSecret=sss;LargeResultDataset=target_dataset;LargeResultTable=target_table;"
mode=bulk
value.converter=com.tigergraph.kafka.connect.converters.TigerGraphCsvConverter


[bq_query_0]
name="bq_query_0"
topic="person-demo"
query="SELECT name, basic.age, basic.gender, state, ARRAY_TO_STRING(tags,'#') FROM `project.dataset.table`"
----

The following is the converted CSV stream:

[source,csv]
----
Tom,40,male,ca,tall#strong
Dan,35,female,ny,smart#blonde
----

The next step is to create the data source.
The following data source configuration file can be used to create the data source:

[source,text]
----
{
    "broker":"10.128.0.240:30002", <1>
}
----
<1> The IP address is the internal network IP address of the server running TigerGraph.

Run the following command to create the data source `k1`:

[source,gsql]
----
CREATE DATA_SOURCE KAFKA k1 FOR GRAPH social
----

The next step is to create the topic-partition configurations to use in the loading job.
In this case, the data source is the `person-demo` topic in TigerGraph's internal Kafka cluster.

[tabs]
====
Topic-partition configurations::
+
--
[,json]
----
{
  "topic": "person-demo",
  "partition_list": [
    {
      "start_offset": -2,
      "partition": 0
    }
  ]
}
----
--
Loading job::
+
--
[.wrap,gsql]
----
CREATE LOADING JOB load_person FOR GRAPH social {
    DEFINE FILENAME f1 = "$k1:/home/mydata/topic_person.json";
    LOAD f1 TO VERTEX person VALUES ($0, $0, $1, $2, $3,SPLIT($4,"#")) USING separator="|";
}
----
--
====

After the loading job has been defined, run the loading job to ingest the data into the graph.