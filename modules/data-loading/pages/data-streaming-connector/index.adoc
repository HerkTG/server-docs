= Data Streaming Connector
:description: A guide to TigerGraph's Streaming Data Connector.
:sectnums:

TigerGraph Data Streaming Connector is a link:https://docs.confluent.io/home/connect/overview.html[Kafka connector] that provides fast and scalable data streaming between TigerGraph and other data systems.

== Architecture overview
Data Streaming Connector streams data from a source data system into TigerGraph's internal Kafka cluster in the form of Kafka messages in specified topics.
The messages are then ingested by a xref:kafka-loader/index.adoc[Kafka loading job] and loaded into the database.

.Data streaming connector architecture
image::data-streaming-connector.png[Data streaming connector streams from data source to TigerGraph's internal Kafka, and a loading job ingests the Kafka messages into the database.]

Multiple connectors can run at the same time, streaming data from various sources into the TigerGraph system concurrently.

== Supported data systems
The data streaming connector supports the following data systems:

* xref:data-streaming-connector/gcp.adoc[Google Cloud Storage (GCS)]
* xref:data-streaming-connector/kafka.adoc[Apache Kafka]
* xref:data-streaming-connector/aws-s3.adoc[AWS S3]
* xref:data-streaming-connector/azure-blob.adoc[Azure Blob Storage (ABS)]
* xref:data-streaming-connector/big-query.adoc[Google BigQuery]


== Manage connectors

After creating a connector, you can choose to delete it or pause it.
You can also use `gadmin` commands to view status of all existing connectors.

=== List connectors
You can list all running connectors or view detailed configuration of a specific connector.

To view a list of all connectors, run `gadmin connector list` from your bash terminal as the TigerGraph Linux user.

To view configuration details of a specific connector, run `gadmin connector list <name_of_connector>` and replace `<name_of_connector>` with the name of the connector you want to inspect.
By default, this command displays most configurations of the connector, but redacts  the credentials used to authenticate the connector.

If you use `gadmin config` to change the configuration parameter `Controller.Connect.RedactSensitiveConfigValue` to `false` , the command displays the connector's authentication credentials.

=== Pause a connector
Pausing a connector stops the connector from streaming data from the data source.
The data that has been streamed to TigerGraph's internal Kafka can still be loaded into the graph store via a loading job.

To pause a connector, run the below command and replace `<connector_name>` with the name of the connector:

[,console]
----
$ gadmin connector pause <connector_name>
----

=== Resume a connector
Resuming a connector resumes streaming for a paused connector.

To resume a connector, run the below command and replace `<connector_name>` with the name of the connector:

[,console]
----
$ gadmin connector resume <connector_name>
----

=== Restart a connector
Restarting a connector is equivalent to pausing and then resuming a connector.

To restart a connector, run the below command and replace `<connector_name>` with the name of the connector:

[,console]
----
$ gadmin connector restart <connector_name>
----

=== Delete a connector
Deleting a connector removes a connector.
It stops the connector from streaming, as well as deletes the data that has been streamed to the corresponding topics in Kafka that haven't been ingested by a loading job.
See the diagram in Figure 1 for details.


This does not delete the Kafka topics themselves, just the data in the topics associated to the connector.
This also does not affect the data that has already been loaded into the database by a loading job, it only deletes the messages in TigerGraph's internal Kafka server.

This action cannot be undone and a removed connector cannot be resumed.

When you create the same connector, with the same connector name and configurations, the connector loads all data from the source again to the same topics.


CAUTION: There is a known issue for deleting connectors with created with folder URIs.
Deleting the connector does not delete the connect offsets for topics that are mapped to a folder URI.
If you create the same connector, with the same name and same URI to a folder, the connector only loads from where it left off before.
A workaround for this issue is to create a connector with the same configurations, but with a different name.

To delete a connector,  run the below command and replace `<connector_name>` with the name of the connector:

[,console]
----
$ gadmin connector delete <connector_name> -y <1>
----
<1> You can use the `-y` option to skip the confirmation prompt.

==== Save Kafka messages when deleting connector
When there are new files in a folder the connector streams from, you might want to recreate a connector for those file updates to be included.
When you delete and recreate a connector, you can keep the messages that have already been streamed to the internal Kafka cluster.
The recreated connector streams from where it left off before and does not duplicate data.

Use the `-s` or `--soft` flag to delete the connector only and leave the Kafka messages and offsets intact:

[.wrap,console]
----
$ gadmin connector delete <connector_name> -s
----

== Create the loading job

The loading job creation happens entirely in GSQL.

=== Specify a graph and config file

[source.wrap, gsql]
----
USE GRAPH test_graph
CREATE DATA_SOURCE STREAM s1 = "ds_config.json" for graph test_graph
----

In this example, we create a data source of type `STREAM` and name it `s1`.
We set our data source as the `ds_config.json` file.

.JSON config file examples
[tabs]
====
Amazon S3::
+
--
[source.wrap, json]
----
{
    "type": "s3",
    "access.key": "<access key>",
    "secret.key": "<secret key>"
}
----
--
Google Cloud Storage::
+
--
[source.wrap, json]
----
{
  "type": "gcs",
  "project_id": "<project id>",
  "private_key_id": "<private key id>",
  "private_key": "<private key>",
  "client_email": "<email address>"
}
----
--
Azure Blob Storage::
+
--
[source.wrap, json]
----
{
    "type" : "abs",
    "account.key" : "<acount key>"
}
----
--
====

Key names accept a separator of either `.` or `_`, so for example, a key could also be named `access_key`.

=== Create the loading job

[source, gsql]
----
LOAD file_Comment
    TO VERTEX Comment VALUES ($"id", $"creationDate", $"locationIP", $"browserUsed", $"content", $"length"),
    TO EDGE HAS_CREATOR VALUES ($"id", $"CreatorPersonId")
    USING JSON_FILE="TRUE";
----

=== Define the filename

[source,gsql]
----
DEFINE FILENAME file_name = "[data source name]:[URI];
DEFINE FILENAME file_name = "[data source name]:[json config file];
DEFINE FILENAME file_name = "[data source name]:[json content];
----

Here are some examples with remote URIs:

[source, gsql]
----
DEFINE FILENAME uri_s3 = "$s1:s3://s3-loading-test/tg_ldbc_snb/sf0.1_csv/dynamic/Comment";
DEFINE FILENAME uri_gcs = "$s1:gs://tg_ldbc_snb/sf0.1_csv/dynamic/Person";
DEFINE FILENAME uri_abs = "$s1:abfss://person@yandblobstorage.dfs.core.windows.net/persondata.csv;"
----

=== Define the parameters

These are the parameters that should be in the JSON configuration file.

xfile.uris  - required, the URI or URI split by comma.

file.type - text for csv and json, parquet for parquet file.

partition - When load data, each partition is distributed evenly across each node. If one FILENAME name container much more data than others, consider using larger parition number for it, default value is ceiling(nodes / number of filenames)

eol - End of line character. default is from loading job and is \n by default

header - default is from loading job and is false

=== Sample loading job syntax

In this example, the dataset is the LDBC_SNB dataset, which has a

[source.wrap, gsql]
----
USE GRAPH test_graph
CREATE DATA_SOURCE STREAM s1 = "ds_config.json" for graph test_graph

CREATE LOADING JOB stream_csv FOR GRAPH test_graph {
    DEFINE FILENAME file_Comment = "$s1:s3://s3-loading-test/tg_ldbc_snb/sf0.1_csv/dynamic/Comment";
    DEFINE FILENAME file_Person = "$s1:s3://s3-loading-test/tg_ldbc_snb/sf0.1_csv/dynamic/Person";
    DEFINE FILENAME file_Comment_hasCreator_Person = "$s1:s3://s3-loading-test/tg_ldbc_snb/sf0.1_csv/dynamic/Comment_hasCreator_Person";
    LOAD file_Comment
    TO VERTEX Comment VALUES ($1, $0, $2, $3, $4, $5) USING header="true", separator="|";
    LOAD file_Person
    TO VERTEX Person VALUES ($1, $2, $3, $4, $5, $0, $6, $7, SPLIT($8,";"), SPLIT($9,";")) USING header="true", separator="|";
    LOAD file_Comment_hasCreator_Person
    TO EDGE HAS_CREATOR VALUES ($1, $2) USING header="true", separator="|";
}
----

=== Run the loading job

Use the command `RUN LOADING JOB` to run the loading job.

[source, gsql]
----
RUN LOADING JOB stream_csv
----

By default, new files or new lines in files in an external data source are not automatically loaded into TigerGraph once the loading job stops.
The data streaming connector also supports continuous loading with an `EOF` flag, short for "terminate loading at the end of the file."

If you run this command with the `EOF` flag set to `false`, the loading job is kept active and any new data in the external data source will be loaded automatically.

[source, gsql]
----
RUN LOADING JOB stream_csv USING EOF="false"
----

[NOTE]
Continuous loading works only on an incremental basis. Only new lines in existing files and new files are loaded with continuous loading.
If any existing lines are changed or deleted, these changes will *not* be part of the loading job.

== Known issues
Messages in TigerGraph's internal Kafka cluster are automatically removed from the topics at regular intervals.
There are several known issues with this process:

* Messages are only removed if the loading job is actively running.
If the loading job finishes much sooner before the interval is reached, the messages are not removed.
* If loading job uses EOF mode, meaning the loading job will terminate as soon as it finishes, it is likely some partial data will be left in the topic.
* If a topic is deleted and recreated while a loading job on the topic is running, the data in the topic may get removed.
* Deleting the connector does not delete the connect offsets for topics that are mapped to a folder URI.

Automatic message removal is an alpha feature and may be subject to change.