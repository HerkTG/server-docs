= Data Streaming Connector

:description: A guide to TigerGraph's Streaming Data Connector.

TigerGraph Data Streaming Connector is a link:https://docs.confluent.io/home/connect/overview.html[Kafka connector] that provides fast and scalable data streaming between TigerGraph and AWS, Azure, and GCP.

For data coming from a separate Kafka instance or in Google BigQuery, you need to perform a few more steps to set up data streaming.
Each of these pages has its own separate instructions.

* xref:data-streaming-connector/kafka.adoc[Apache Kafka]
* xref:data-streaming-connector/big-query.adoc[Google BigQuery]

== Architecture overview
The connector streams data from a source data system into TigerGraph's internal Kafka cluster in the form of Kafka messages in specified topics.
The messages are then ingested by a Kafka loading job and loaded into the database.

.Data streaming connector architecture
image::data-streaming-connector.png[Data streaming connector streams from data source to TigerGraph's internal Kafka, and a loading job ingests the Kafka messages into the database.]

Multiple connectors can run at the same time, streaming data from various sources into the TigerGraph system concurrently.

TigerGraph automatically sets up the streaming connector and Kafka job when an external data source is specified during loading job creation.

== Create the loading job

Loading jobs for the Data Streaming Connector are created and run in GSQL.

=== Specify a graph and config file

[source.wrap, gsql]
----
USE GRAPH test_graph
CREATE DATA_SOURCE STREAM s1 = "ds_config.json" FOR GRAPH test_graph
----

In this example, we create a data source of type `STREAM` and name it `s1`.
We associate our data source with the `ds_config.json` file.

.JSON config file examples
[tabs]
====
Amazon S3::
+
--
[source.wrap, json]
----
{
    "type": "s3",
    "access.key": "<access key>",
    "secret.key": "<secret key>"
}
----
--
Google Cloud Storage::
+
--
[source.wrap, json]
----
{
  "type": "gcs",
  "project_id": "<project id>",
  "private_key_id": "<private key id>",
  "private_key": "<private key>",
  "client_email": "<email address>"
}
----
--
Azure Blob Storage::
+
--
[source.wrap, json]
----
{
    "type" : "abs",
    "account.key" : "<acount key>"
}
----
--
====

Key names accept a separator of either `.` or `_`, so for example, a key could also be named `access_key`.

=== Create the loading job

[source, gsql]
----
LOAD file_Comment
    TO VERTEX Comment VALUES ($"id", $"creationDate", $"locationIP", $"browserUsed", $"content", $"length"),
    TO EDGE HAS_CREATOR VALUES ($"id", $"CreatorPersonId")
    USING JSON_FILE="TRUE";
----

=== Define the filename

The filename should be in one of these three formats:

[source,gsql]
----
DEFINE FILENAME file_name = "[data source name]:[URI];
DEFINE FILENAME file_name = "[data source name]:[json config file];
DEFINE FILENAME file_name = "[data source name]:[inline json content];
----

Here are some examples with remote URIs.
If the filename is in URI format and refers to a folder or prefix, all files in that folder or with that prefix are loaded.

[source, gsql]
----
DEFINE FILENAME uri_s3 = "$s1:s3://s3-loading-test/tg_ldbc_snb/sf0.1_csv/dynamic/Comment";
DEFINE FILENAME uri_gcs = "$s1:gs://tg_ldbc_snb/sf0.1_csv/dynamic/Person";
DEFINE FILENAME uri_abs = "$s1:abfss://person@yandblobstorage.dfs.core.windows.net/persondata.csv;"
----

=== Define the parameters

These are the parameters that should be in the JSON configuration file.

* `xfile.uris`  - required, the URI or URIs split by comma.

* `file.type` - `text` for CSV and JSON, `parquet` for Parquet file.

* `partition` - When loading data, each partition is distributed evenly across each node.
If one `FILENAME` name contains much more data than others, consider using larger partition number for it.
The default value is calculated by `ceiling(nodes / number of filenames)`

* `eol` - End of line character.
The default is `\n`.

* `header` - Whether or not the data contains a header line at the start.
The default is `false`.

=== Sample loading job syntax

In this example, the dataset is the LDBC_SNB dataset, which has a

[source.wrap, gsql]
----
USE GRAPH test_graph
CREATE DATA_SOURCE STREAM s1 = "ds_config.json" for graph test_graph

CREATE LOADING JOB stream_csv FOR GRAPH test_graph {
    DEFINE FILENAME file_Comment = "$s1:s3://s3-loading-test/tg_ldbc_snb/sf0.1_csv/dynamic/Comment";
    DEFINE FILENAME file_Person = "$s1:s3://s3-loading-test/tg_ldbc_snb/sf0.1_csv/dynamic/Person";
    DEFINE FILENAME file_Comment_hasCreator_Person = "$s1:s3://s3-loading-test/tg_ldbc_snb/sf0.1_csv/dynamic/Comment_hasCreator_Person";
    LOAD file_Comment
    TO VERTEX Comment VALUES ($1, $0, $2, $3, $4, $5) USING header="true", separator="|";
    LOAD file_Person
    TO VERTEX Person VALUES ($1, $2, $3, $4, $5, $0, $6, $7, SPLIT($8,";"), SPLIT($9,";")) USING header="true", separator="|";
    LOAD file_Comment_hasCreator_Person
    TO EDGE HAS_CREATOR VALUES ($1, $2) USING header="true", separator="|";
}
----

=== Run the loading job

Use the command `RUN LOADING JOB` to run the loading job.

[source, gsql]
----
RUN LOADING JOB stream_csv
----

==== Continuous file loading

By default, after a loading job stops, changes to files in an external data source are not automatically loaded into TigerGraph.

The data streaming connector also supports continuous loading in stream mode.
This is controlled with the `EOF` flag.
If the `EOF` flag is set to `true`, the continuous loading will stop when the loader encounters an end-of-file (EOF) character in the data.

If you run this command with the `EOF` flag set to `false`, the loading job is kept active and any new data in the external data source will be loaded automatically.

[source, gsql]
----
RUN LOADING JOB stream_csv USING EOF="false"
----

[NOTE]
Continuous loading works only on an incremental basis. Only new lines in existing files and new files are loaded with continuous loading.
If any existing lines are changed or deleted, these changes will *not* be part of the loading job.

For example, consider a file `data.txt` in cloud storage that is part of a loading job.

.data.txt
[source,text]
----
line-1
----

The line of data is loaded successfully into the loading job for ingestion to TigerGraph.
If a user edits the file and adds a new line, the stream loader notices the new modification and loads new lines, starting from where it previously left off.
The actual data on each line is not compared to what was already loaded.

.data.txt after a new line is added to the end
[source,text]
----
line-1
line-2
----

In this case, the new line `line-2` is successfully loaded into the loading job for ingestion to TigerGraph.

If a user edits the file and adds a line before the end, like so, the entire file is loaded again, causing potentially repeated data.

.data.txt after a new line is added before the end
[source,text]
----
line-1
added-line
line-2
----

The data loaded into TigerGraph thus looks like this.
Because two lines had already been loaded, the first two lines are skipped, even though the second contains new data.
The third line from the file is then loaded, resulting in a repeat of what was already loaded in the last pass.

.Data in TigerGraph
[source,text]
----
line-1
line-2
line-2
----

To avoid this, only use stream loading jobs when there is no chance of data being altered or added to the middle of a file.


== Known issues
Messages in TigerGraph's internal Kafka cluster are automatically removed from the topics at regular intervals.
There are several known issues with this process:

* Messages are only removed if the loading job is actively running.
If the loading job finishes much sooner before the interval is reached, the messages are not removed.
* If loading job uses EOF mode, meaning the loading job will terminate as soon as it finishes, it is likely some partial data will be left in the topic.
* If a topic is deleted and recreated while a loading job on the topic is running, the data in the topic may get removed.
* Deleting the connector does not delete the connect offsets for topics that are mapped to a folder URI.

Automatic message removal is an alpha feature and may be subject to change.