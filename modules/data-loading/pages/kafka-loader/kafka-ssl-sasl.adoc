= Set up SSL/SASL for Kafka Loader
:description:

TigerGraph's Kafka loader supports encrypting data in-transit using SSL and SASL protocols.
This page details how to set up

* Set up one-way SSL
* Set up two-way SSL
* Set up SASL
* Set up SSL and SASL

== Set up one-way SSL
You can set up SSL to only have the server side (the external Kafka cluster) verify the identity of the client (TigerGraph).

=== Before you begin
The following prerequisites should be met on the server side:

* Ensure you have link:https://docs.confluent.io/platform/current/kafka/authentication_ssl.html[enabled SSL encryption for your Kafka cluster].
** For one-way SSL, you do not need to configure truststore properties in `server.properties`.
Instead, you supply truststore information when you configure data source for the kafka loader.
* Ensure you have created a key for every broker with the Common Name (CN) matching th the fully qualified domain name (FQDN) of the server.
* Ensure you have created a Certificate Authority(CA).
* Ensure that you have signed the certificates with the CA and imported both the certificate of the CA and the signed certificate into the broker keystore:.
* Add the full qualified domain name of the server to `/etc/hosts`

You will need the following files and passwords in order to configure SSL on TigerGraph.

* The truststore file from the broker
* The broker's Certificate Authority

=== Procedure
. Copy configuration files and certificates from broker to TigerGraph servers.
Below is the list of files to copy:
* The truststore file from the broker
** For example, `server.truststore.jks`
* The broker's Certificate Authority
. xref:kafka-loader/kafka-ssl-sasl.adoc[Create the Kafka data source] and specify SSL configurations in the datasource configuration JSON file.
For example:
+
[.wrap,json]
----
{
   "broker": "kafka-0.tigergraph.com:9092", <1>
   "kafka_config":{
       "security.protocol": "SSL",
       "ssl.endpoint.identification.algorithm": "",
       "ssl.ca.location":"/home/tigergraph/SSL_one_way/ca-root.crt",
       "ssl.truststore.location":"/home/tigergraph/SSL_one_way/server.truststore.jks", <2>
       "ssl.truststore.password":"tiger123"
   }
}
----
<1> The hostname of the broker.
<2> `ssl.ca.location` and `ssl.truststore.location` should match the paths where you copied the certificate authority and the truststore file.

Once you have defined the data source with the correct SSL configuration, you have configured one-way SSL.
When you run loading jobs from this data source, SSL secures data in transit and the client verifies the identity of the server.


== Set up two-way SSL

You can set up two-way SSL between TigerGraph and your external Kafka cluster and have the client authenticate the server and vice versa.

=== Before you begin
* Ensure you have link:https://docs.confluent.io/platform/current/kafka/authentication_ssl.html[enabled SSL encryption for your Kafka cluster].

=== Procedure
. Copy the following files from the broker to every node:
* The broker's CA
* The client's certificate signed by the broker's CA
* The client's key
* The broker's truststore file
* The broker's keystore file
. xref:kafka-loader/kafka-ssl-sasl.adoc[Create the Kafka data source] and specify SSL configurations in the datasource configuration JSON file.
The location of the CA, the truststore and the keystore files should match the paths where you copied the files.
For example:
+
[.wrap,json]
----
{
   "broker": "kafka-0.tigergraph.com:9092",
   "kafka_config":{
       "security.protocol": "SSL",
       "ssl.endpoint.identification.algorithm": "",
       "ssl.ca.location":"/home/tigergraph/SSL_two_way/ca-root.crt",
       "ssl.certificate.location":"/home/tigergraph/SSL_two_way/tigergraph_client.crt",
       "ssl.key.location":"/home/tigergraph/SSL_two_way/tigergraph_client.key",
       "ssl.key.password":"tiger123",
       "ssl.keystore.location":"/home/tigergraph/SSL_two_way/server.keystore.jks",
       "ssl.keystore.password":"tiger123",
       "ssl.truststore.location":"/home/tigergraph/SSL_two_way/server.truststore.jks",
       "ssl.truststore.password":"tiger123"
   }
}
----

Once you have defined the data source with the correct SSL configuration, you have configured two-way SSL.
When you run loading jobs from this data source, SSL secures data in transit and both the server and the client authenticate each other.

== Set up SASL with GSSAPI

=== Before you begin
.On broker
* Ensure you have link:https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_gssapi.html[configured SASL with GSSAPI] on the broker.
* Ensure the IP address and the hostname of the broker is in the `hosts` file in the `/etc` directory of the OS.

.On client
* Make sure the following packages are installed on the client (TigerGraph) server.
** On Centos:
+
[,console]
----
yum install krb5-workstation
yum install cyrus-sasl-gssapi
----
** On Ubuntu/Debian
+
[,console]
----
apt install krb5-user
apt install libsasl2-modules-gssapi-mit
apt install libsasl2-modules-gssapi-heimdal <1>
----
<1> You only need to install `libsasl2-modules-gssapi-heimdal` if you are on Ubuntu 20.04 LTS

=== Procedure
. Add the IP address and the hostname of the broker to the `/etc/hosts` file.
. Define the data source and provide SASL configurations in the data source configuration file.
For example:
+
[,json]
----
{
   "broker": "kafka-0.tigergraph.com:9092",
   "kafka_config":{
       "security.protocol": "SASL_PLAINTEXT",
       "sasl.mechanism": "GSSAPI",
       "sasl.kerberos.service.name":"kafka", <1>
       "sasl.kerberos.principal": "kafka-producer@TIGERGRAPH.COM", <2>
       "sasl.kerberos.keytab": "/home/tigergraph/kafka_ssl/kafka-producer.keytab",
       "sasl.jaas.config": "com.sun.security.auth.module.Krb5LoginModule required  debug=true useKeyTab=true  storeKey=true  keyTab=\"/home/tigergraph/kafka_ssl/kafka-producer.keytab\"  principal=\"kafka-producer@TIGERGRAPH.COM\";"
   }
}
----
<1> `sasl.kerberos.service.name` needs to match
<2> `sasl.kerberos.principal` needs to match the principal value in the broker's JAAS configuration file.

Once you have defined the data source with the correct SASL configuration, you have configured SASL with GSSAPI between TigerGraph and your Kafka cluster for Kafka loading.

When you run loading jobs from this data source, the Kafka cluster will authenticate the identity of TigerGraph server.
However, the data in transit remains unencrypted.

== Set up SSL and SASL

You can set up SASL authentication protocol over an SSL-encrypted communication channel.

=== Before you begin
* Follow Confluent documentation to configure link:https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_gssapi.html#brokers[SASL with GSSAPI] on the broker, and specify `security.inter.broker.protocol=SASL_SSL` to be `SASL_SSL`.
This guide focuses on configuring the client (TigerGraph server).

=== Procedure
. Copy the following files from the broker to the client.
* The broker's CA
* The client's certificate signed by the broker's CA
* The client's key
* The broker's truststore
* The broker's keystore
* The Kafka producer's keytab
. On the client server, create a JAAS configuration file `kafka_client_jaas.conf`. In the configuration file, configure the following values:
* Set `com.sun.security.auth.module.Krb5LoginModule` to `required`.
* Set `useKeyTab` to `true`.
* Set `storeKey` to `true`.
* Set `keyTab` to the path where copied the producer keytab file.
* Set `principal` to the producer principal.
+
[,text]
----
KafkaClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    storeKey=true
    keyTab="/home/tigergraph/kafka_ssl/kafka-producer.keytab"
    principal="kafka-producer@TIGERGRAPH.COM";
};
----
. Define the Kafka data source with the following configuration:
+
[,javascript]
----
{
   "broker": "kafka-0.tigergraph.com:9092",
   "kafka_config":{
       "security.protocol": "SASL_SSL",
       "sasl.mechanism": "GSSAPI",
       "sasl.kerberos.service.name":"kafka",
       "ssl.endpoint.identification.algorithm": "",
       "ssl.ca.location": <path_to_ca>,
       "ssl.certificate.location":<path_to_client_certificate>,
       "ssl.key.location":<path_to_client_key>,
       "ssl.key.password": <password_for_key>,
       "ssl.keystore.location":<path_to_server_keystore>,
       "ssl.keystore.password":<keystore_password>,
       "ssl.truststore.location":<path_to_server_trsutstore>,
       "ssl.truststore.password":<truststore_password>,
       "sasl.kerberos.principal": <producer_principal_name>,
       "sasl.kerberos.keytab": <path_to_pro>,
       "sasl.jaas.config": "com.sun.security.auth.module.Krb5LoginModule required  debug=true useKeyTab=true  storeKey=true  keyTab=\"/home/tigergraph/kafka_ssl/kafka-producer.keytab\"  principal=\"kafka-producer@TIGERGRAPH.COM\";" <1>
   }
}
----
<1> `sasl.jaas.config` shares the same content as the JAAS configuration file on the client.

Once you have defined the data source with the correct SASL and SSL configuration, you have configured SASL with GSSAPI between TigerGraph and your Kafka cluster for Kafka loading.
Communication between TigerGraph and your external Kafka cluster uses SASL authentication protocol over SSL-encrypted communication channel.